{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import TensorShape\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model, save_model, model_from_json\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, categorical_crossentropy, kullback_leibler_divergence\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import xmltodict\n",
    "import multiprocessing\n",
    "\n",
    "from itertools import combinations \n",
    "from datetime import datetime\n",
    "\n",
    "#np.random.seed(1)\n",
    "#tf.random.set_seed(2)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config_file = \"data\\\\generated_data\\\\kampff\\\\2015_09_03_Cell.9.0_gt0\\\\data_config.xml\"\n",
    "\n",
    "with open(data_config_file) as fd:\n",
    "    data_config = xmltodict.parse(fd.read())\n",
    "    data_config = data_config[\"data\"] \n",
    "    \n",
    "## load the training config file in the future instead global parameters below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network parameter(s)\n",
    "batch_size = 8192\n",
    "global_dropout = .5\n",
    "lstm_dropout = 0.5\n",
    "conv_dropout = 0.5\n",
    "\n",
    "image_shape = [int(data_config[\"timespan\"]),128] \n",
    "\n",
    "use_multi_gpu = False\n",
    "only_binary_labels = False\n",
    "only_positives = True\n",
    "\n",
    "## Loss parameters\n",
    "alpha = 1024 # reconstruction loss constant ( this is usually the input image size )\n",
    "beta = 15 # for the B-VAE\n",
    "delta = 2 # for the binary crossvalidation\n",
    "theta = 1 # for the cluster closeness  \n",
    "ce_eq = 1# for equalizing the cross_entropy loss \n",
    "\n",
    "## Latent parameters\n",
    "latent_dim = 32\n",
    "upper_latent_dim = 8\n",
    "\n",
    "\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True) \n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[1], True)  \n",
    "## Training parameter(s)\n",
    "\n",
    "epochs = 200\n",
    "validation_freq= 5\n",
    "cluster_update_interval = 10 # per epochs  \n",
    "target_dist = []\n",
    "\n",
    "load_VAE_from_file = True\n",
    "fine_tune_detector = True # if fine tune is True we will load the detector weights\n",
    "train_only_detector = False  # this trains the detector only\n",
    "load_only_encoder = False\n",
    "load_full_model = False\n",
    "\n",
    "use_clustering = True\n",
    "train_detector = True or train_only_detector # cce is valid_loss ?\n",
    "use_reconstruction_loss = True\n",
    "\n",
    "max_clusters = int(data_config[\"max_clusters\"])+1 # cluster number\n",
    "if only_binary_labels:\n",
    "    max_clusters = min(max_clusters, 2)\n",
    "#Step parameters\n",
    "training_steps = int(int(data_config[\"train\"][\"samples\"]) / batch_size) \n",
    "validation_steps = int(int(data_config[\"val\"][\"samples\"]) / batch_size)  \n",
    "test_steps = int(int(data_config[\"test\"][\"samples\"]) / batch_size)\n",
    "\n",
    "## Visualization parameter(s)\n",
    "epoch_per_visualization = 25\n",
    "step_per_visualization = 500\n",
    "\n",
    "filter_train_to_ratio = True\n",
    "## Fine tuning options \n",
    "                \n",
    "pretrained_model_path = \"trained_models\\\\kampff\\\\2015_09_03_Cell.9.0_gt0\\\\pre_trained_vae_\"#\"trained_models\\\\pre_trained_vae_\"\n",
    "full_model_path = \"trained_models\\\\vae_model.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parsing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fn(proto):\n",
    "    features = {'image': tf.io.FixedLenFeature([], tf.string), 'label': tf.io.FixedLenFeature([], tf.int64)}\n",
    "    parsed_features = tf.io.parse_single_example(proto, features)\n",
    "    images = tf.io.decode_raw(parsed_features['image'], tf.int16)\n",
    "    labels = tf.cast(parsed_features['label'], tf.uint8)\n",
    "    \n",
    "    if only_binary_labels:\n",
    "        \n",
    "        labels = tf.cast(labels, tf.int32)\n",
    "        labels = tf.math.minimum(labels, 1)\n",
    "        labels = tf.one_hot(labels, depth=2)\n",
    "    \n",
    "    else:\n",
    "        labels = tf.one_hot(labels, depth=max_clusters)\n",
    "    \n",
    "    images = tf.reshape(images, image_shape)\n",
    "    \n",
    "    images = tf.cast(images, tf.float32)\n",
    "\n",
    "    images = (images / tf.math.reduce_max(images))\n",
    "   \n",
    "    return (images,labels), (images, labels, labels, [0], [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "def resampler_class_func(initial_rate, target_rate=50.):\n",
    "    \n",
    "    initial_rate = ops.convert_to_tensor(initial_rate, name=\"initial_rate\")\n",
    "    target_rate = ops.convert_to_tensor(target_rate, name=\"target_rate\")\n",
    "\n",
    "    rate = target_rate / initial_rate # 25 \n",
    "    prob_rate = 1.0 / rate\n",
    "    \n",
    "    \n",
    "    if initial_rate > 50.:\n",
    "        prob_rate = 1.0\n",
    "    \n",
    "    def _map_fn(data, label):\n",
    "        \n",
    "        label = tf.argmax(data[1])\n",
    "        label= tf.cast(label, tf.float32)\n",
    "        random_num = tf.random.uniform(shape=[1])\n",
    "        \n",
    "        bin_layer = tf.math.reduce_min([label,1])\n",
    "        \n",
    "        prob_rate_for_sample = [(1-bin_layer)*prob_rate] + bin_layer\n",
    "    \n",
    "        if random_num <= prob_rate_for_sample:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    \n",
    "    return _map_fn\n",
    "\n",
    "def random_flip_left_right( data, label):\n",
    "\n",
    "    img, label = data\n",
    "    img = tf.expand_dims(img, -1)\n",
    "    img_flipped = tf.image.random_flip_left_right(img, 5)\n",
    "    img_flipped = tf.squeeze(img_flipped, -1)\n",
    "    \n",
    "    return (img_flipped, label), (img_flipped, label, label, [0], [0])\n",
    "\n",
    "def only_positive_filter():\n",
    "\n",
    "    def _map_fn(data, label):\n",
    "\n",
    "        return tf.argmax(label[1]) > 0\n",
    "\n",
    "    return _map_fn\n",
    "    \n",
    "def get_dataset(fname, samples=4096, pos_rate=None):\n",
    "    \n",
    "    samples = int(samples) # \n",
    "    training_file = os.path.join(data_config[\"dir\"],fname)\n",
    "\n",
    "    # Build an iterator over training batches.\n",
    "    training_dataset = tf.data.TFRecordDataset(training_file)\n",
    "\n",
    "    training_dataset = training_dataset.map(parse_fn, num_parallel_calls=3)\n",
    "    \n",
    "    \n",
    "    if pos_rate is not None and filter_train_to_ratio:\n",
    "        training_dataset = training_dataset.filter(resampler_class_func(pos_rate)) \n",
    "        \n",
    "    if only_positives and max_clusters > 2 and not only_binary_labels:\n",
    "        training_dataset = training_dataset.filter(only_positive_filter())\n",
    "\n",
    "    \n",
    "    training_batches = training_dataset.take(samples).shuffle(8192, reshuffle_each_iteration=True).batch(batch_size)#.prefetch(min(1024, batch_size*4)).cache()\n",
    "    \n",
    "    #sample = tf.reshape(sample, shape=image_shape)\n",
    "    \n",
    "    return training_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(input, filter):\n",
    "    x3 = Conv2D(filter, kernel_size=3, padding=\"same\", activation=\"relu\")(input)\n",
    "    x5 = Conv2D(filter, kernel_size=5, padding=\"same\", activation=\"relu\")(input)\n",
    "    x1 = Conv2D(filter, kernel_size=1, padding=\"same\", activation=\"relu\")(input)\n",
    "\n",
    "    concat = Concatenate(axis=-1)([x1,x3,x5])\n",
    "\n",
    "    return Conv2D(filter, kernel_size=1, padding=\"same\", activation=\"relu\")(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClusteringLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        \n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        \n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.   \n",
    "        \n",
    "        return q  \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var - K.square(mu) - K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(beta * K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    def from_config(cls, config):\n",
    "        return cls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LatentLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        super(LatentLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        z_mean, z_log_var = x\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "       \n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        \n",
    "        \n",
    "        z_mean, z_log_var = KLDivergenceLayer()([z_mean, z_log_var])\n",
    "        \n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape_a, shape_b = input_shape\n",
    "        return shape_a\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    def from_config(cls, config):\n",
    "        return cls()\n",
    "    def from_config(config):\n",
    "        return LatentLayer()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the classificator auxiliary branch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_classificator(input_shape=latent_dim+upper_latent_dim):\n",
    "    inputs = Input(shape=(input_shape,), name='latent_dim_detector_input')\n",
    "\n",
    "    x = Dropout(global_dropout)(inputs)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(global_dropout)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(global_dropout)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(global_dropout)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    \n",
    "    x = Dense(max_clusters, activation=\"softmax\")(x)\n",
    "    classificator = Model(inputs, x, name='detection')\n",
    "    \n",
    "    return classificator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_model(inputs):\n",
    "    \n",
    "    x = Reshape((image_shape[0]*image_shape[1],))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Reshape((image_shape[0], image_shape[1]))(x)\n",
    "    \n",
    "    normalized_input = x\n",
    "    \n",
    "    #\"\"\"\n",
    "    conv_x = Reshape((image_shape[0], 4, 32))(x)\n",
    "\n",
    "    conv_x = tf.transpose(conv_x, perm=[0, 3, 2, 1])\n",
    "\n",
    "    conv_x = lenet(conv_x, 32)\n",
    "    \n",
    "    conv_x = Dropout(conv_dropout)(conv_x)\n",
    "    \n",
    "    conv_x = lenet(conv_x, 64)\n",
    "    \n",
    "    conv_x = Dropout(conv_dropout)(conv_x)\n",
    "\n",
    "    conv_x = Conv2D(64, kernel_size=2, padding=\"valid\", activation=\"relu\")(conv_x)\n",
    "\n",
    "    conv_x = lenet(conv_x, 128)\n",
    "    \n",
    "    conv_x = Dropout(conv_dropout)(conv_x)\n",
    "\n",
    "    conv_x = Conv2D(128, kernel_size=2, padding=\"valid\", activation=\"relu\")(conv_x)\n",
    "\n",
    "    conv_x = lenet(conv_x, 256)\n",
    "\n",
    "    conv_x = Dropout(conv_dropout)(conv_x)\n",
    "    \n",
    "    conv_x = Flatten()(conv_x)\n",
    "    #\"\"\"   \n",
    "    \n",
    "    blstm = Bidirectional(LSTM(128, return_sequences=True, dropout=lstm_dropout))(x)\n",
    "    \n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=lstm_dropout))(blstm)\n",
    "    x = Attention()([blstm, x])\n",
    "    \n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=lstm_dropout))(x)\n",
    "    \n",
    "    blstm = Bidirectional(LSTM(64, return_sequences=True, dropout=lstm_dropout))(x)\n",
    "    x = Attention()([blstm, x])\n",
    "    \n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, dropout=lstm_dropout))(x)\n",
    "    \n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Concatenate(axis=1)([x, conv_x])\n",
    "    x = Dense(latent_dim*2, activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    #x = Dropout(rate=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    \n",
    "    \n",
    "    z = LatentLayer()([z_mean, z_log_var])\n",
    "       \n",
    "    # \n",
    "    # Upper latent encoding \n",
    "    #\n",
    "    \n",
    "    upper_z = Dense(16, activation=\"relu\")(z)\n",
    "    \n",
    "    \n",
    "    upper_z = BatchNormalization()(upper_z)\n",
    "    upper_mean = Dense(upper_latent_dim, name=\"upper_mean\")(upper_z)\n",
    "    upper_log_var = Dense(upper_latent_dim, name=\"upper_log_var\")(upper_z)\n",
    "    \n",
    "    upper_z = LatentLayer()([upper_mean, upper_log_var])\n",
    "    \n",
    "    \n",
    "    final_z = Concatenate(axis=1, name=\"final_z_layer\")([z,upper_z])\n",
    "    # We need to concatenate the mean and the std also so we can calc the gradients for the upper latents aswell\n",
    "    final_z_mean = Concatenate(axis=1)([z_mean, upper_mean])    \n",
    "    final_z_log_var = Concatenate(axis=1)([z_log_var, upper_log_var])\n",
    "\n",
    "    \n",
    "    \n",
    "    encoder = Model(inputs, final_z, name='encoder')\n",
    "    #encoder.summary()\n",
    "\n",
    "    return encoder, final_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_model(input_shape=latent_dim+upper_latent_dim):\n",
    "    \n",
    "    latent_inputs = Input(shape=(input_shape,), name='z_sampling')\n",
    "    \n",
    "    x = Dense(image_shape[0]*image_shape[1], activation='relu')(latent_inputs)\n",
    "\n",
    "    x = Reshape((image_shape[0], image_shape[1]))(x)\n",
    "    \n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, dropout=lstm_dropout))(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=lstm_dropout))(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=lstm_dropout))(x)\n",
    "    \n",
    "    blstm = Bidirectional(LSTM(128, return_sequences=True, dropout=lstm_dropout))(x)\n",
    "    x = Attention()([x, blstm])\n",
    "    \n",
    "    outputs = LSTM(128, return_sequences=True, dropout=lstm_dropout)(x)\n",
    "\n",
    "    outputs = Reshape((image_shape[0], image_shape[1]))(outputs)\n",
    "\n",
    "    decoder = Model(latent_inputs, outputs, name='reconstruction')\n",
    "\n",
    "    \n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(\"train.tfrecord\", samples=data_config[\"train\"][\"samples\"], pos_rate=float(data_config[\"train\"][\"ratio\"]))\n",
    "val_dataset = get_dataset(\"val.tfrecord\", samples=data_config[\"val\"][\"samples\"])\n",
    "test_dataset = get_dataset(\"test.tfrecord\", samples=data_config[\"test\"][\"samples\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall, Precision and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcF1Score(data, steps, mode=\"micro\"): # or macro or weighted or all\n",
    "    iterator = tf.compat.v1.data.make_one_shot_iterator(data)\n",
    "    y_t = []\n",
    "    y_p = []\n",
    "    for s in range(steps):\n",
    "\n",
    "        gt = iterator.get_next()\n",
    "        batch_im = gt[0]\n",
    "        y_true = gt[1][1]\n",
    "\n",
    "        output, y_pred, _, _, _ = vae.predict(batch_im, steps=1)\n",
    "\n",
    "        y_t.extend(y_true)\n",
    "        y_p.extend(y_pred)\n",
    "\n",
    "    y_t = np.asarray(y_t)\n",
    "    y_p = np.asarray(y_p)\n",
    "    y_t = np.argmax(y_t, axis=1)\n",
    "    y_p = np.argmax(y_p, axis=1)\n",
    "    \n",
    "    if mode == \"all\":\n",
    "        \n",
    "        mic = f1_score(np.asarray(y_t), np.asarray(y_p), average=\"micro\")\n",
    "        mac = f1_score(np.asarray(y_t), np.asarray(y_p), average=\"macro\")\n",
    "        wei = f1_score(np.asarray(y_t), np.asarray(y_p), average=\"weighted\")\n",
    "        \n",
    "        print(\"F1 micro: \"+str(mic))\n",
    "        print(\"F1 macro: \"+str(mac))\n",
    "        print(\"F1 weighted: \"+str(wei))\n",
    "        \n",
    "        return mic, mac, wei\n",
    "    \n",
    "    res = f1_score(np.asarray(y_t), np.asarray(y_p), average=mode)\n",
    "    print(\"F1 \"+mode+\": \"+str(res))\n",
    "    return res\n",
    "\n",
    "def calcCompletness(data, steps): # or macro or weighted\n",
    "    iterator = tf.compat.v1.data.make_one_shot_iterator(data)\n",
    "    y_t = []\n",
    "    y_p = []\n",
    "    for s in range(steps):\n",
    "\n",
    "        gt = iterator.get_next()\n",
    "        batch_im = gt[0]\n",
    "        y_true = gt[1][1]\n",
    "\n",
    "        output, y_pred, _, _,_ = vae.predict(batch_im, steps=1)\n",
    "\n",
    "        y_t.extend(y_true)\n",
    "        y_p.extend(y_pred)\n",
    "\n",
    "    y_t = np.asarray(y_t)\n",
    "    y_p = np.asarray(y_p)\n",
    "    y_t = np.argmax(y_t, axis=1)\n",
    "    y_p = np.argmax(y_p, axis=1)\n",
    "    \n",
    "    homogeneity, completness, vmes = homogeneity_completeness_v_measure(y_t, y_p)\n",
    "    print(\"Completness: : \"+str(completness))\n",
    "    print(\"homogeneity: : \"+str(homogeneity))\n",
    "    print(\"V measure: : \"+str(vmes))\n",
    "    return homogeneity, completness, vmes\n",
    "\n",
    "def calcEverything(data, steps):\n",
    "    y_t = []\n",
    "    y_p = []\n",
    "    \n",
    "    output, y_p, _, _,y_t = vae.predict(data, steps=steps)\n",
    "\n",
    "\n",
    "    y_t = np.asarray(y_t)\n",
    "    y_p = np.asarray(y_p)\n",
    "    y_t = np.argmax(y_t, axis=1)\n",
    "    y_p = np.argmax(y_p, axis=1)\n",
    "    \n",
    "    mic = f1_score(np.asarray(y_t), np.asarray(y_p), average=\"micro\")\n",
    "    mac = f1_score(np.asarray(y_t), np.asarray(y_p), average=\"macro\")\n",
    "    wei = f1_score(np.asarray(y_t), np.asarray(y_p), average=\"weighted\")\n",
    "        \n",
    "    print(\"F1 micro: \"+str(mic))\n",
    "    print(\"F1 macro: \"+str(mac))\n",
    "    print(\"F1 weighted: \"+str(wei))\n",
    "    \n",
    "    homogeneity, completness, vmes = homogeneity_completeness_v_measure(y_t, y_p)\n",
    "    print(\"Completness: : \"+str(completness))\n",
    "    print(\"homogeneity: : \"+str(homogeneity))\n",
    "    print(\"V measure: : \"+str(vmes))\n",
    "    \n",
    "    return mic, mac, wei, completness, homogeneity, vmes\n",
    "\n",
    "def calcRecallPrecision(data, steps):\n",
    "    iterator = tf.compat.v1.data.make_one_shot_iterator(data)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for s in range(steps):\n",
    "\n",
    "        gt = iterator.get_next()\n",
    "        batch_im = gt[0]\n",
    "        y_true = gt[1][1]\n",
    "\n",
    "        output, y_pred, _, _,_ = vae.predict(batch_im, steps=1)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            if y_true[b][0] == np.round(y_pred[b][0]):\n",
    "                if y_true[b][0] == 1:\n",
    "                    tn +=1\n",
    "                else:\n",
    "                    tp +=1\n",
    "            else:\n",
    "                if y_true[b][0] == 1:\n",
    "                    fp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "                \n",
    "    print ((tp, tn, fp, fn))\n",
    "    return (100*tp/(fn+tp+sys.float_info.epsilon), 100*tp/(fp+tp+sys.float_info.epsilon))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the custom losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(valid_loss = True):\n",
    "    def loss(original, reconstructed):\n",
    "        \n",
    "        reconstruction_loss = mse(K.flatten(original), K.flatten(reconstructed))\n",
    "        reconstruction_loss *= alpha\n",
    "\n",
    "        if not valid_loss:\n",
    "            return 0.\n",
    "\n",
    "        return K.mean(reconstruction_loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def custom_crossentropy(use_delta=True, valid_loss = True):\n",
    "    def loss(y_true, y_pred):\n",
    "        if not valid_loss:\n",
    "            return 0.\n",
    "        cce = categorical_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        loss_ = ce_eq * (1-y_true[:,0])* cce + y_true[:,0]*cce \n",
    "        \n",
    "        if use_delta:\n",
    "            loss_ = delta * loss_\n",
    "            \n",
    "            \n",
    "        return loss_\n",
    "\n",
    "    return loss\n",
    "\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return 0.\n",
    "\n",
    "def cluster_loss(use_theta = True, valid_loss = True):\n",
    "    \n",
    "    def loss(y_true, layer_output):\n",
    "        \n",
    "        if not valid_loss:\n",
    "            return 0.\n",
    "        \n",
    "        if use_theta:\n",
    "            layer_output = theta * kullback_leibler_divergence(y_true, layer_output)\n",
    "        \n",
    "        return layer_output\n",
    "    \n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_loss_to_load(use_vae_loss = True, use_detector_loss = True):\n",
    "    def loss(y_true, y_pred):\n",
    "        if y_true.shape[-1] == 2:\n",
    "            if use_detector_loss:\n",
    "                return custom_crossentropy(use_delta=use_vae_loss)(y_true, y_pred)\n",
    "            else:\n",
    "                return 0.\n",
    "        else:\n",
    "            if use_vae_loss:\n",
    "                return vae_loss()(y_true, y_pred)\n",
    "            else:\n",
    "                return 0.\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate = 0.001)\n",
    "\n",
    "def build_model(load_weights = False, freeze_ae=False):\n",
    "\n",
    "    if use_multi_gpu:     \n",
    "\n",
    "        cross_device_ops = tf.distribute.HierarchicalCopyAllReduce(num_packs=1)\n",
    "        \n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=cross_device_ops).scope()\n",
    "    else:\n",
    "        mirrored_strategy = nullcontext()\n",
    "        mirrored_strategy = tf.device('/gpu:0')\n",
    "    \n",
    "    with mirrored_strategy:\n",
    "        inputs = Input(image_shape, name=\"input\")\n",
    "\n",
    "        inputs_aux = Input((max_clusters,), name=\"input_labels\")\n",
    "\n",
    "        encoder, latent_layer = encoder_model(inputs)\n",
    "    \n",
    "        decoder = decoder_model()\n",
    "    \n",
    "        if load_weights:\n",
    "            \n",
    "            encoder.load_weights(pretrained_model_path+\"encoder.h5\") \n",
    "            if not load_only_encoder:\n",
    "                decoder.load_weights(pretrained_model_path+\"decoder.h5\")\n",
    "        \n",
    "        outputs = decoder(latent_layer)\n",
    "        \n",
    "    \n",
    "        if freeze_ae:\n",
    "            encoder.trainable = False\n",
    "            if not load_only_encoder:\n",
    "                decoder.trainable = False\n",
    "\n",
    "            beta = 0.\n",
    "    \n",
    "        detector = aux_classificator()\n",
    "    \n",
    "        if fine_tune_detector:\n",
    "            detector.load_weights(pretrained_model_path+\"detector.h5\")\n",
    "        \n",
    "        detector_output = detector(latent_layer)    \n",
    "    \n",
    "        cluster_output = ClusteringLayer(max_clusters, name=\"clustering\")(latent_layer)\n",
    "    \n",
    "        vae = Model([inputs, inputs_aux], [outputs, detector_output, cluster_output, latent_layer, inputs_aux], name='vae')\n",
    "        vae.compile(optimizer=optimizer, metrics={\"detection\":[\"accuracy\"]}, loss=[vae_loss(valid_loss=(not freeze_ae or load_only_encoder or use_reconstruction_loss)), custom_crossentropy(use_delta = (not freeze_ae or load_only_encoder), valid_loss=train_detector), cluster_loss(valid_loss=use_clustering), zero_loss, zero_loss])\n",
    "\n",
    "    vae.summary()\n",
    "    \n",
    "    return vae, encoder, decoder, detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/build model if its the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 64, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 8192)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 8192)         32768       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 64, 128)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 64, 4, 32)    0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(None, 32, 4, 64)]  0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 4, 32)    2080        tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 4, 32)    18464       tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 4, 32)    51232       tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 4, 96)    0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 4, 32)    3104        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32, 4, 32)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 4, 64)    2112        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 4, 64)    18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 4, 64)    51264       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 4, 192)   0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 4, 64)    12352       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 4, 64)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 31, 3, 64)    16448       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 31, 3, 128)   8320        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 31, 3, 128)   73856       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 31, 3, 128)   204928      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 31, 3, 384)   0           conv2d_11[0][0]                  \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 64, 256)      263168      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 31, 3, 128)   49280       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64, 256)      394240      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 31, 3, 128)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 64, 256)      0           bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 30, 2, 128)   65664       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 64, 128)      164352      attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 30, 2, 256)   33024       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 30, 2, 256)   295168      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 30, 2, 256)   819456      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 64, 128)      98816       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 30, 2, 768)   0           conv2d_16[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 64, 128)      0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 30, 2, 256)   196864      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 64, 64)       41216       attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 30, 2, 256)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 15360)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 19456)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           1245248     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           256         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 32)           2080        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 32)           2080        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "latent_layer (LatentLayer)      (None, 32)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           528         latent_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "upper_mean (Dense)              (None, 8)            136         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "upper_log_var (Dense)           (None, 8)            136         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "latent_layer_1 (LatentLayer)    (None, 8)            0           upper_mean[0][0]                 \n",
      "                                                                 upper_log_var[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_z_layer (Concatenate)     (None, 40)           0           latent_layer[0][0]               \n",
      "                                                                 latent_layer_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reconstruction (Model)          (None, 64, 128)      1297664     final_z_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "detection (Model)               (None, 2)            193602      final_z_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "clustering (ClusteringLayer)    (None, 2)            80          final_z_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_labels (InputLayer)       [(None, 2)]          0                                            \n",
      "==================================================================================================\n",
      "Total params: 5,658,546\n",
      "Trainable params: 5,642,002\n",
      "Non-trainable params: 16,544\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if load_VAE_from_file:\n",
    "    K.clear_session()\n",
    "    if load_full_model:\n",
    "        vae = load_model(full_model_path, custom_objects={\"ClusteringLayer\":ClusteringLayer, \"LatentLayer\":LatentLayer, \"AttentionLayer\":AttentionLayer, \"loss\":custom_loss_to_load(), \"zero_loss\":custom_loss_to_load()})\n",
    "    else:\n",
    "        if train_only_detector: # load the model, the loss will be calculated only for the detector\n",
    "            vae, encoder, decoder, detector = build_model(load_weights = True, freeze_ae = True)\n",
    "        \n",
    "        elif use_clustering:\n",
    "            vae, encoder, decoder, detector = build_model(load_weights = True, freeze_ae = False)\n",
    "        else: # load the entire model with loss being applied to all of the outputs\n",
    "            vae, encoder, decoder, detector = build_model(load_weights = True)\n",
    "else: \n",
    "    vae, encoder, decoder, detector = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining custom callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Images callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowImages(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, data, time_dim_ = False):\n",
    "    self.data = data \n",
    "    self.time_dim = time_dim_\n",
    "\n",
    "    \n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    return\n",
    "    #print('\\nTraining: epoch {} begins at {}'.format(epoch, datetime.datetime.now().time()))\n",
    "\n",
    "  def on_train_end(self, logs=None):\n",
    "    self.on_epoch_end(0)\n",
    "    return\n",
    "  \n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    \n",
    "    current_epoch = epoch\n",
    "    if(epoch != 0 and (epoch+1) % epoch_per_visualization != 0):\n",
    "        return\n",
    "    \n",
    "    iterator = tf.compat.v1.data.make_one_shot_iterator(self.data)\n",
    "    try:\n",
    "        gt = iterator.get_next()\n",
    "    except Exception:\n",
    "        return\n",
    "    \n",
    "    im = gt[0]\n",
    "    label = gt[1][1]\n",
    "    \n",
    "    output, detected, latent_layer, _, _= vae.predict(im, steps=1)\n",
    "    im_norm = im[0]\n",
    "    \n",
    "    num_to_plot = 10\n",
    "    fig=plt.figure(figsize=(num_to_plot, 12))\n",
    "   \n",
    "    if(self.time_dim):\n",
    "        \n",
    "        # Plotting in time dimension \n",
    "        print (tf.keras.backend.get_value(label))\n",
    "        for i in [np.argmax(tf.keras.backend.get_value(label)[:, 1])]:#range(num_to_plot):\n",
    "            orig = im_norm[0,:,i]\n",
    "            predicted = output[0,:,i]\n",
    "\n",
    "            fig.add_subplot(num_to_plot, 2, 2*i+1)\n",
    "            plt.title(\"GT\")\n",
    "            plt.plot(orig)\n",
    "            bottom, top = plt.ylim()\n",
    "\n",
    "            fig.add_subplot(num_to_plot, 2, 2*i+2)\n",
    "            plt.title(\"PR\")\n",
    "            plt.plot(predicted)\n",
    "            plt.ylim(bottom, top)\n",
    "\n",
    "        plt.subplots_adjust(hspace=0.2)\n",
    "        plt.subplots_adjust(wspace=1)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Plotting in space dim\n",
    "        \n",
    "        for i in range(num_to_plot):\n",
    "\n",
    "            orig = im_norm[i] # FOR CONV3D: +1 dim HERE\n",
    "            predicted = output[i] # FOR CONV3D: +1 dim HERE\n",
    "            \n",
    "            \n",
    "            fig.add_subplot(num_to_plot, 2, 2*i+1)\n",
    "            \n",
    "            plt.title(\"Ground truth\")\n",
    "                \n",
    "            plt.imshow(orig)\n",
    "            cbar = plt.colorbar(aspect=5)\n",
    "            cbar.ax.get_yaxis().set_ticks([0,1])\n",
    "            \n",
    "            vmin = np.min(orig)\n",
    "            vmax = np.max(orig)\n",
    "            fig.add_subplot(num_to_plot, 2, 2*i+2)\n",
    "\n",
    "            plt.title(\"Predicted\")\n",
    "            \n",
    "            plt.imshow(predicted, vmin=vmin, vmax=vmax)\n",
    "            cbar = plt.colorbar(aspect=5)\n",
    "            cbar.ax.get_yaxis().set_ticks([0,1])\n",
    "            \n",
    "\n",
    "        plt.subplots_adjust(hspace=1)\n",
    "        plt.show()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateClusters(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, data, iter_num = training_steps):\n",
    "        self.data = data \n",
    "        self.iter_num = iter_num\n",
    "        self.n_clusters = max_clusters\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if not use_clustering:\n",
    "            return\n",
    "        \n",
    "        if epoch % cluster_update_interval != 0:\n",
    "            return\n",
    "            \n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        \n",
    "        _, _, _, latent,_ = vae.predict(train_dataset, steps=200) # for large datasets we need to limit \n",
    "        \n",
    "        kmeans.fit(latent)\n",
    "        \n",
    "        vae.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta-scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaScheduler(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, low = -2, high = 2, freq = 20, enabled=True):\n",
    "        self.low = low \n",
    "        self.high = high\n",
    "        self.freq = freq\n",
    "        self.use_scheduler = enabled\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if not self.use_scheduler:\n",
    "            return\n",
    "        \n",
    "        inside_step = epoch % self.freq\n",
    "        \n",
    "        if inside_step < self.freq //2:\n",
    "            beta = (self.high/((self.freq//2)**2))*inside_step*inside_step\n",
    "        else:\n",
    "            beta = self.high\n",
    "        \n",
    "        if epoch % self.freq != 0:\n",
    "            return\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "si = ShowImages(test_dataset, time_dim_=False)\n",
    "si.on_epoch_end(0)\n",
    "\n",
    "if False:\n",
    "    for i in range(20):\n",
    "        si.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Fit function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                              patience=10, min_lr=0.00001, cooldown=5, verbose=1)\n",
    "\n",
    "logdir = \"logs\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "save_checkpoint = ModelCheckpoint(\"trained_models\\\\checkpoint\\\\checkpoint.hdf5\", monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "def fit(num_epochs):\n",
    "    return vae.fit(train_dataset,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=None,\n",
    "                validation_data=(val_dataset, None, None),\n",
    "                validation_steps=None,\n",
    "                validation_freq = validation_freq,\n",
    "                verbose=1,\n",
    "                callbacks=[ShowImages(val_dataset), reduce_lr, UpdateClusters(train_dataset), tensorboard_callback, BetaScheduler() ],#, save_checkpoint],\n",
    "                use_multiprocessing=True,\n",
    "                workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 64, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 8192)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 8192)         32768       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 64, 128)      0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 64, 4, 32)    0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(None, 32, 4, 64)]  0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 4, 32)    2080        tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 4, 32)    18464       tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 4, 32)    51232       tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 4, 96)    0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 4, 32)    3104        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32, 4, 32)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 4, 64)    2112        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 4, 64)    18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 4, 64)    51264       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 4, 192)   0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 4, 64)    12352       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 4, 64)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 31, 3, 64)    16448       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 31, 3, 128)   8320        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 31, 3, 128)   73856       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 31, 3, 128)   204928      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 31, 3, 384)   0           conv2d_11[0][0]                  \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 64, 256)      263168      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 31, 3, 128)   49280       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64, 256)      394240      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 31, 3, 128)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 64, 256)      0           bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 30, 2, 128)   65664       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 64, 128)      164352      attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 30, 2, 256)   33024       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 30, 2, 256)   295168      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 30, 2, 256)   819456      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 64, 128)      98816       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 30, 2, 768)   0           conv2d_16[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 64, 128)      0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 30, 2, 256)   196864      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 64, 64)       41216       attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 30, 2, 256)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 15360)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 19456)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           1245248     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           256         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 32)           2080        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 32)           2080        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "latent_layer (LatentLayer)      (None, 32)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           528         latent_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "upper_mean (Dense)              (None, 8)            136         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "upper_log_var (Dense)           (None, 8)            136         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "latent_layer_1 (LatentLayer)    (None, 8)            0           upper_mean[0][0]                 \n",
      "                                                                 upper_log_var[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_z_layer (Concatenate)     (None, 40)           0           latent_layer[0][0]               \n",
      "                                                                 latent_layer_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reconstruction (Model)          (None, 64, 128)      1297664     final_z_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "detection (Model)               (None, 2)            193602      final_z_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "clustering (ClusteringLayer)    (None, 2)            80          final_z_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_labels (InputLayer)       [(None, 2)]          0                                            \n",
      "==================================================================================================\n",
      "Total params: 5,658,546\n",
      "Trainable params: 5,642,002\n",
      "Non-trainable params: 16,544\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a471fe8e7ab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#encoder.trainable = False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "#decoder.trainable = False\n",
    "#detector.trainable = True\n",
    "#encoder.trainable = False\n",
    "vae.summary()\n",
    "a = 2/0\n",
    "history = fit(epochs)\n",
    "encoder.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_VAE_from_file:\n",
    "    # save the weight of AE\n",
    "    encoder.save_weights(pretrained_model_path+\"encoder.h5\")\n",
    "    decoder.save_weights(pretrained_model_path+\"decoder.h5\")\n",
    "    \n",
    "detector.save_weights(pretrained_model_path+\"detector.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model(vae, full_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae.evaluate(test_dataset)\n",
    "#vae.evaluate(val_dataset, steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Recall and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_recall, train_precision = calcRecallPrecision(train_dataset, training_steps)\n",
    "print(\"Train dataset: \")\n",
    "print(\"---- Recall: \"+str(train_recall))\n",
    "print(\"---- Precision: \"+str(train_precision))\n",
    "print(\"----------------------------------\\n\")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "val_recall, val_precision = calcRecallPrecision(val_dataset, validation_steps)\n",
    "print(\"Val dataset: \")\n",
    "print(\"---- Recall: \"+str(val_recall))\n",
    "print(\"---- Precision: \"+str(val_precision))\n",
    "print(\"----------------------------------\\n\")\n",
    "#\"\"\"\n",
    "\"\"\"\n",
    "test_recall, test_precision = calcRecallPrecision(test_dataset, test_steps)\n",
    "print(\"Test dataset: \")\n",
    "print(\"---- Recall: \"+str(test_recall))\n",
    "print(\"---- Precision: \"+str(test_precision))\n",
    "#\"\"\"\n",
    "\n",
    "#print(\"\\nResults for Train \")\n",
    "#calcEverything(train_dataset, training_steps)\n",
    "\n",
    "#print(\"\\nResults for Validation \")\n",
    "#calcEverything(val_dataset, validation_steps)\n",
    "\n",
    "print(\"\\nResults for Test \")\n",
    "calcEverything(test_dataset, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing with the encoder only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
      "x faster than real life:  160.72593322359063\n",
      "Ratio:  34.44596103734955\n",
      "(8059,)\n",
      "(8059, 40)\n",
      "(8059,)\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "latents = []\n",
    "\n",
    "import time \n",
    "t1 = time.time()\n",
    "_, detection, _, latents, gt = vae.predict(train_dataset, steps=100)\n",
    "\n",
    "runtime= time.time()-t1\n",
    "print(\"x faster than real life: \", (batch_size*100*image_shape[0])/(20000*runtime))\n",
    "\n",
    "#detection = detection[:,1]\n",
    "#detection = [np.where(r>=0.5)[0][0] for r in detection]\n",
    "#detection = np.asarray(detection)\n",
    "#detection = np.round(detection)\n",
    "\n",
    "detection = np.argmax(detection, axis=1)\n",
    "\n",
    "gt = np.argmax(gt, axis=1)\n",
    "#gt = detection # to see what was detected \n",
    "\n",
    "#gt = gt == detection\n",
    "\n",
    "print(\"Ratio: \", len(gt[gt==1])*100/len(gt))\n",
    "print(gt.shape)\n",
    "print(latents.shape)\n",
    "print(detection.shape)\n",
    "print(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=3, verbose=1, perplexity=25, n_iter=300, learning_rate=50)\n",
    "\n",
    "\n",
    "latents = np.nan_to_num(latents)\n",
    "\n",
    "from numpy import inf\n",
    "for l in range(len(latents)):\n",
    "    \n",
    "    latents[l] = np.nan_to_num(latents[l])\n",
    "    latents[l][latents[l] == inf] = 0\n",
    "    latents[l][latents[l] == -inf] = 0\n",
    "\n",
    "latents = latents.astype(\"float64\")\n",
    "    \n",
    "latents_pca = pca.fit_transform(latents) \n",
    "latents_pca = tsne.fit_transform(latents)\n",
    "#latents_pca = latents[:,3:8]\n",
    "\n",
    "#latents_pca = latents[:, :3]\n",
    "cdict= {0:\"blue\", 1 :\"red\"}\n",
    "sample_num = latents.shape[0]\n",
    "print(max_clusters)\n",
    "ax = plt.axes(projection='3d')\n",
    "for cat in range(max_clusters):\n",
    "    lat = np.where(gt[:sample_num] == cat)\n",
    "    for i in range(0,latents_pca.shape[1]-1,2):\n",
    "        ax.scatter3D(latents_pca[lat,i,], latents_pca[lat,i+1], latents_pca[lat,i+2], c = np.random.rand(3,))\n",
    "\n",
    "#ax.scatter3D(latents[:sample_num,0], latents[:sample_num,1])\n",
    "ax.view_init(0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
